{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":3948,"databundleVersionId":32624,"sourceType":"competition"}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **시간대별 자전거수요예측**\n2025 - 02 - 02","metadata":{}},{"cell_type":"markdown","source":"## **(1) 데이터 탐색 및 전처리**","metadata":{}},{"cell_type":"markdown","source":"### **a.데이터 불러오기**","metadata":{}},{"cell_type":"code","source":"# 데이터 로드\nimport pandas as pd\nimport numpy as np\n\ntrain_df = pd.read_csv(\"/kaggle/input/bike-sharing-demand/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/bike-sharing-demand/test.csv\")\nsubmission = pd.read_csv(\"/kaggle/input/bike-sharing-demand/sampleSubmission.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T12:58:39.555752Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.head(5)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df.head(5)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"###  **b.데이터 기본정보 학인**","metadata":{}},{"cell_type":"code","source":"train_df.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.describe()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"###  **C.전처리**","metadata":{}},{"cell_type":"code","source":"# 각 컬럼의 유니크값 확인\nprint(\"======== 유니크값 개수\")\nprint(train_df.nunique())\n\nprint(\"======== 각 변수 유니크값 상세 보기\")\nfor column in train_df.columns:\n    print(f\"\\n{column} unique values:\")\n    print(train_df[column].value_counts().sort_index())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 날짜 데이터 처리 datetime 파싱\nfor df in [train_df, test_df]:\n    df['datetime'] = pd.to_datetime(df['datetime'])\n    # 시간 관련 특성 추출\n    df['year'] = df['datetime'].dt.year\n    df['month'] = df['datetime'].dt.month\n    df['day'] = df['datetime'].dt.day\n    df['hour'] = df['datetime'].dt.hour\n    df['dayofweek'] = df['datetime'].dt.dayofweek\n    df['is_rush_hour'] = df['hour'].apply(lambda x: 1 if x in [8,9,17,18] else 0)\n    df['is_weekend'] = df['dayofweek'].isin([5,6]).astype(int)\n\nprint(train_df[['datetime','year','month','hour','dayofweek']].head(5))\nprint(test_df[['datetime','year','month','hour','dayofweek']].head(5))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 새로 생성된 시간 특성의 유니크값 확인\nprint(\"======== 생성된 시간 특성 유니크값\")\ntime_features = ['year', 'month', 'day', 'hour', 'dayofweek', 'is_weekend']\nfor feature in time_features:\n    print(f\"\\n{feature} unique values:\")\n    print(train_df[feature].value_counts().sort_index())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 범주형 변수 처리  (원-핫 인코딩)\n# 'holiday', 'workingday' 이미 이진화처리 되어 있음.\n\n# train_df = pd.get_dummies(train_df, columns=['season', 'weather'])\n# test_df = pd.get_dummies(test_df, columns=['season', 'weather'])\n\n# 'weather' 컬럼에 대한 원-핫 인코딩\nweather_encoded = pd.get_dummies(train_df['weather'], prefix='weather')\ntest_weather_encoded = pd.get_dummies(test_df['weather'], prefix='weather')\n\n# 원본 데이터프레임에 인코딩된 컬럼 추가\ntrain_df = pd.concat([train_df, weather_encoded], axis=1)\ntest_df = pd.concat([test_df, test_weather_encoded], axis=1)\n\n# 'season' 컬럼에 대한 원-핫 인코딩\nseason_encoded = pd.get_dummies(train_df['season'], prefix='season')\ntest_season_encoded = pd.get_dummies(test_df['season'], prefix='season')\n\n# 원본 데이터프레임에 인코딩된 컬럼 추가\ntrain_df = pd.concat([train_df, season_encoded], axis=1)\ntest_df = pd.concat([test_df, test_season_encoded], axis=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 결측치 확인\nprint(\"======== Train 결측치 확인\")\nprint(train_df.isnull().sum())\nprint(\"======== Test 결측치 확인\")\nprint(test_df.isnull().sum())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 수치형 변수 전처리\nfor df in [train_df, test_df]:\n    df['windspeed_log'] = np.log1p(df['windspeed'])\n    df['temp_diff'] = df['temp'] - df['atemp']\n    df['temp_humidity'] = df['temp'] * df['humidity']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!apt-get update -qq\n!apt-get install fonts-nanum* -qq","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!fc-cache -fv","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 수치형 변수 분포 확인\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nfig, axes = plt.subplots(2, 1, figsize=(12, 10))\nsns.boxplot(data=train_df[['temp','atemp','humidity','windspeed']], ax=axes[0])\naxes[0].set_title('Train Data Numerical Variable Distribution') # 한글 대신 영어로 제목 변경ㅜㅜ\nsns.boxplot(data=test_df[['temp','atemp','humidity','windspeed']], ax=axes[1])\naxes[1].set_title('Test Data Numerical Variable Distribution')\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"1. 온도 관련 변수\n* temp(온도): 0-40도 사이에 분포하며, 중앙값은 약 20도입니다. 분포가 비교적 고르며 극단적인 이상치가 있어보이지는 않음.\n* atemp(체감온도): temp와 유사한 분포를 보이며, 실제 온도와 비슷한 범위에서 변동하고 있는것으로 보여짐.\n\n2. 기상 조건 변수\n* humidity(습도): 0-100% 사이에 분포하며, 중앙값은 약 60%입니다. 박스의 크기가 크므로 변동성이 큰 편임.\n* windspeed(풍속): 대부분의 값이 0-20 사이에 분포하며, 다수의 이상치(outlier)가 존재. 특히 30-60 범위에 여러 극단값들이 있어보임.","metadata":{}},{"cell_type":"code","source":"# train과 test의 컬럼 일치 확인\nprint(\"======== 컬럼 비교\")\nprint(\"Train columns:\", train_df.columns.tolist())\nprint(\"Test columns:\", test_df.columns.tolist())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 새로 생성된 특성들의 기술통계량 비교\nprint(\"======== 새로운 특성 통계량 비교\")\nnew_features = ['windspeed_log', 'temp_diff', 'temp_humidity']\nprint(\"======== Train:\")\nprint(train_df[new_features].describe())\nprint(\"======== Test:\")\nprint(test_df[new_features].describe())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **(2) EDA**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\n\n# 그래프 스타일 설정\nplt.style.use('seaborn')\nsns.set_palette(\"husl\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"###  **2-a.시간대별 자전거 대여량 분석**\n* 러시아워(출퇴근 시간) 패턴 확인\n* 주간/야간 대여량 차이 파악\n","metadata":{}},{"cell_type":"code","source":"# 1. 시간대별 자전거 대여량 분석\nplt.figure(figsize=(12, 6))\nsns.boxplot(x='hour', y='count', data=train_df)\nplt.title('Hourly Rental Distribution')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"###  **2-b.요일별 자전거 대여량 분석**\n* 주중/주말 패턴 차이\n* 특정 요일의 대여량 특징","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nsns.boxplot(x='dayofweek', y='count', data=train_df)\nplt.title('Daily Rental Distribution')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"###   **2-c.상관관계 분석**\n* 변수간 상관관계 강도 확인\n* 다중공선성 가능성 체크","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12, 8))\nnumeric_cols = df.select_dtypes(include=[np.number]).columns\nsns.heatmap(df[numeric_cols].corr(), annot=True, cmap='RdYlBu_r', center=0)\nplt.title('Correlation Matrix')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"###  **2-d.날씨와 대여량의 관계**\n* 날씨 상태별 대여량 차이\n* 극단적 날씨의 영향","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12, 6))\nsns.boxplot(x='weather', y='count', data=train_df)\nplt.title('Weather Impact on Rentals')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"###   **2-e.온도와 대여량의 산점도**\n* 온도와 대여량의 관계\n* 최적 대여 온도 구간 파악","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nsns.scatterplot(x='temp', y='count', data=train_df, alpha=0.5)\nplt.title('Temperature vs Rental Count')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"###  **2-f.월별 대여량 추이**\n* 월별 대여량 패턴\n* 계절성 확인","metadata":{}},{"cell_type":"code","source":"monthly_rentals = train_df.groupby('month')['count'].mean()\nplt.figure(figsize=(12, 6))\nmonthly_rentals.plot(kind='bar')\nplt.title('Average Monthly Rentals')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"###  **2-g.시간대별 평균 대여량**\n* 일중 대여량 패턴\n* 피크 시간대 확인","metadata":{}},{"cell_type":"code","source":"hourly_rentals = train_df.groupby('hour')['count'].mean()\nplt.figure(figsize=(12, 6))\nhourly_rentals.plot(kind='line', marker='o')\nplt.title('Average Hourly Rentals')\nplt.grid(True)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##  **(3) 데이터 분리**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import TimeSeriesSplit\n\n# 전처리 완료된 train_df와 test_df가 있다고 가정\n# datetime 인덱스 설정 (시간 순서 유지)\ntrain_df['datetime'] = pd.to_datetime(train_df['datetime'])\ntrain_df.set_index('datetime', inplace=True)\ntrain_df.sort_index(inplace=True)\n\n# 특성과 타겟 분리\nX = train_df.drop(['casual', 'registered', 'count'], axis=1)\ny = train_df['count']\n\n# 시계열 교차 검증 분할 (5개 폴드)\ntscv = TimeSeriesSplit(n_splits=5)\nfor fold, (train_index, val_index) in enumerate(tscv.split(X), 1):\n    print(f\"Fold {fold}:\")\n    print(f\"  Train: {X.iloc[train_index].index.min()} ~ {X.iloc[train_index].index.max()}\")\n    print(f\"  Val  : {X.iloc[val_index].index.min()} ~ {X.iloc[val_index].index.max()}\")\n\n# 최종 분할 (마지막 폴드 사용)\ntrain_index, val_index = list(tscv.split(X))[-1]\nX_train, X_val = X.iloc[train_index], X.iloc[val_index]\ny_train, y_val = y.iloc[train_index], y.iloc[val_index]\n\nprint(\"==========최종 데이터 형태\")\nprint(f\"Train: {X_train.shape}, Val: {X_val.shape}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **(4) 모델 분석 및 평가**","metadata":{}},{"cell_type":"markdown","source":"## **4-a. 랜덤 포레스트**","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_log_error, r2_score\nimport matplotlib.pyplot as plt\n\n# 랜덤 포레스트 모델 학습\nrf_model = RandomForestRegressor(n_estimators=100, random_state=42)\nrf_model.fit(X_train, y_train)\n\n# 예측 및 평가\ny_pred = rf_model.predict(X_val)\n\n# RMSLE 계산\nrmsle = np.sqrt(mean_squared_log_error(y_val, y_pred))\nr2 = r2_score(y_val, y_pred)\n\nprint(f\"Random Forest - RMSLE: {rmsle:.4f}, R2: {r2:.4f}\")\n\n# 특성 중요도 시각화\nfeature_importance = rf_model.feature_importances_\nsorted_idx = np.argsort(feature_importance)\npos = np.arange(sorted_idx.shape[0]) + .5\n\nplt.figure(figsize=(12, 6))\nplt.barh(pos, feature_importance[sorted_idx], align='center')\nplt.yticks(pos, X.columns[sorted_idx])\nplt.title('Feature Importance (Random Forest)')\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **4-b. XGBoost**","metadata":{}},{"cell_type":"code","source":"# y_true(실제 값) 또는 y_pred(예측 값) 중 하나 이상에 음수 값이 포함 되었는지 확인\nprint(\"Minimum y_val:\", y_val.min())\nprint(\"Minimum y_pred:\", y_pred.min())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_log_error, r2_score\nimport matplotlib.pyplot as plt\n\n# XGBoost 모델 초기화\nxgb_model = XGBRegressor(\n    n_estimators=100,\n    learning_rate=0.1,\n    max_depth=7,\n    random_state=42\n)\n\n# 모델 학습\nxgb_model.fit(X_train, y_train)\n\n# 검증 세트에 대한 예측\ny_pred = xgb_model.predict(X_val)\n\n# 음수 예측값 처리\ny_pred = np.maximum(y_pred, 0)\n\n# 평가 지표 계산\nrmsle = np.sqrt(mean_squared_log_error(y_val, y_pred))\nr2 = r2_score(y_val, y_pred)\n\nprint(f\"XGBoost - RMSLE: {rmsle:.4f}, R2: {r2:.4f}\")\n\n# 특성 중요도 시각화\nfeature_importance = xgb_model.feature_importances_\nsorted_idx = np.argsort(feature_importance)\npos = np.arange(sorted_idx.shape[0]) + .5\n\nplt.figure(figsize=(12, 6))\nplt.barh(pos, feature_importance[sorted_idx], align='center')\nplt.yticks(pos, X_train.columns[sorted_idx])\nplt.title('Feature Importance (XGBoost)')\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **(5) XGBoost 하이퍼파라미터 튜닝**","metadata":{}},{"cell_type":"code","source":"xgb_params = {\n    # 트리 관련 파라미터\n    'max_depth': [3, 5, 7, 9],        # 트리의 최대 깊이\n    'min_child_weight': [1, 3, 5],    # 리프 노드의 최소 가중치 합\n    'gamma': [0, 0.1, 0.2],           # 리프 노드를 추가적으로 나눌지 결정하는 최소 손실 감소값\n    \n    # 부스팅 파라미터\n    'learning_rate': [0.01, 0.05, 0.1],  # 학습률\n    'n_estimators': [100, 200, 300],     # 트리의 개수\n    \n    # 샘플링 파라미터\n    'subsample': [0.8, 0.9, 1.0],        # 샘플링할 훈련 데이터의 비율\n    'colsample_bytree': [0.8, 0.9, 1.0]  # 각 트리마다 샘플링할 피처의 비율\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\n\n# RandomizedSearchCV 정의\nrandom_search = RandomizedSearchCV(\n    estimator=xgb_model,\n    param_distributions=xgb_params,\n    n_iter=100,\n    scoring='neg_mean_squared_log_error',  # 수정된 부분\n    cv=5,\n    n_jobs=-1,\n    verbose=2,\n    random_state=42\n)\n\n# 랜덤 서치 수행\nrandom_search.fit(X_train, y_train)\n\n# 최적 파라미터 및 성능 출력\nprint(\"Best parameters:\", random_search.best_params_)\nprint(\"Best RMSLE:\", np.sqrt(-random_search.best_score_))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**-------------------Best RMSLE: 0.6808839833431952 로 안좋아짐 ㅠㅠ**","metadata":{}},{"cell_type":"code","source":"xgb_params = {\n    'max_depth': [5, 6, 7, 8], # 트리의 최대 깊이\n    'min_child_weight': [1, 3], # 리프 노드의 최소 가중치 합\n    'learning_rate': [0.05, 0.1], # 학습률\n    'n_estimators': [200, 300], # 트리의 개수\n    'subsample': [0.8, 0.9],  # 샘플링할 훈련 데이터의 비율\n    'colsample_bytree': [0.8, 0.9] # 각 트리마다 샘플링할 피처의 비율\n}\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\n\nrandom_search = RandomizedSearchCV(\n    estimator=XGBRegressor(random_state=42),\n    param_distributions=xgb_params,\n    n_iter=50,  # 반복 횟수 줄임\n    scoring='neg_mean_squared_log_error',\n    cv=5,\n    n_jobs=-1,\n    verbose=2,\n    random_state=42\n)\n\n# 학습 전 데이터 확인\nprint(\"Data shape:\", X_train.shape)\nprint(\"Any NaN in X:\", np.isnan(X_train).any())\nprint(\"Any NaN in y:\", np.isnan(y_train).any())\n\n# 모델 학습\nrandom_search.fit(X_train, y_train)\n\n# 결과 출력\nprint(\"\\nBest parameters:\", random_search.best_params_)\nprint(\"Best RMSLE:\", np.sqrt(-random_search.best_score_))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"xgb_params = {\n    'max_depth': [4, 5, 6],\n    'min_child_weight': [1, 2, 3],\n    'learning_rate': [0.05, 0.1],\n    'n_estimators': [100, 200],\n    'subsample': [0.8, 0.9],\n    'colsample_bytree': [0.8, 0.9],\n    'gamma': [0, 0.1]\n}\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import make_scorer\nimport numpy as np\n\n# 사용자 정의 RMSLE 함수\ndef custom_rmsle(y_true, y_pred):\n    y_pred = np.maximum(y_pred, 0)  # 음수 예측값을 0으로 변환\n    return np.sqrt(mean_squared_log_error(y_true, y_pred))\n\n# 사용자 정의 scorer\nrmsle_scorer = make_scorer(custom_rmsle, greater_is_better=False)\n\n# RandomizedSearchCV 수정\nrandom_search = RandomizedSearchCV(\n    estimator=XGBRegressor(random_state=42),\n    param_distributions=xgb_params,\n    n_iter=50,\n    scoring=rmsle_scorer,  # 수정된 scorer 사용\n    cv=5,\n    n_jobs=-1,\n    verbose=2,\n    random_state=42\n)\n\n# 모델 학습\nrandom_search.fit(X_train, y_train)\n\n# 결과 출력\nprint(\"\\nBest parameters:\", random_search.best_params_)\nprint(\"Best RMSLE:\", np.sqrt(-random_search.best_score_))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"-----------------------Best RMSLE: 0.7266514177559081 ㅜㅜ 왜 .... 더 안좋은데","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}